# 🧠 Mercor AI Text Detection — Ensemble Learning Pipeline

This repository contains a complete *stacking and blending ensemble* solution for the [Mercor AI Text Detection](https://www.kaggle.com/competitions/mercor-ai-text-detection) Kaggle competition.  
It combines traditional machine learning models with meta-learning to achieve strong leaderboard performance.

---

## 🚀 Overview

The goal of the competition is to identify whether a given answer was generated by AI or written by a human.  
Our approach focuses on:
- *TF-IDF vectorization* with n-gram features  
- *Multiple base models* trained on text + engineered features  
- *Stacking and blending* to improve generalization  
- *Reproducible results* with organized notebooks and outputs

---

## 🧠 Ensemble Architecture

**Text Data → TF-IDF Vectorization → [LR | RF | XGB | LGBM] → Meta-Learner (LightGBM) → Weighted Blending → Final Submission**

This multi-stage ensemble leverages diversity among traditional ML models and meta-learning to capture both lexical and statistical text patterns.

---

## ⚙ Pipeline Highlights

| Stage | Description |
|-------|--------------|
| *1. Data Preprocessing* | Cleaning text, feature engineering (text_length, num_words, punctuation counts, etc.) |
| *2. TF-IDF Vectorization* | 1–2 gram TF-IDF embeddings (max 8,000 features) |
| *3. Base Models* | Logistic Regression, Random Forest, XGBoost, LightGBM |
| *4. Meta-Model (Stacking)* | LightGBM / Logistic Regression trained on base model outputs |
| *5. Blending* | Weighted average of stacked + blended outputs |

---

## 🧩 Results

| Metric | Score |
|---------|--------|
| *Best Model* | Stacked LightGBM Meta-Learner |
| *Public Leaderboard* | *0.95384* |
| *Validation Strategy* | 5-Fold Stratified Cross-Validation |

*Final generated submissions:*
- stacked_submission.csv
- stacked_submission_2.csv
- super_blend_submission.csv

📅 Project completed: October 2025

---

## 📂 Project Structure
```
Mercor_AI_Text_Detection/
│
├── Mecor_Data/
│ ├── train.csv
│ ├── test.csv
│
├── 01_baseline_ensemble.ipynb
├── requirements.txt
├── .gitignore
└── README.md
```
---

## 📥 Data Setup

The Kaggle dataset is *not included* due to licensing restrictions.

To reproduce results:
1. Download the dataset from the [Mercor AI Text Detection competition](https://www.kaggle.com/competitions/mercor-ai-text-detection).  
2. Create the folder structure shown above.  
3. Run the notebooks — everything else (preprocessing, training, and blending) will execute automatically.

---

## 🏁 Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/Mercor_AI_Text_Detection.git
```
---

# Install dependencies
pip install -r requirements.txt

---

# Run notebook
jupyter notebook 01_baseline_ensemble.ipynb

---

## ⚙️ Reproducibility Details

| Parameter | Value |
|------------|--------|
| **Random Seed** | 42 |
| **Cross-Validation** | StratifiedKFold (n_splits=5) |
| **TF-IDF Settings** | ngram_range=(1,2), max_features=8000 |
| **Environment** | Python 3.10, scikit-learn 1.5+, LightGBM 4.3+, XGBoost 2.1+ |

Blending weights were tuned via **grid search** on out-of-fold predictions to minimize **log loss** and maximize leaderboard stability.

---

## 🔬 Technical Notes

- **Feature engineering included:**
  - Text statistics (`text_length`, `avg_word_length`, `num_stopwords`, etc.)
  - Stylometric ratios (e.g., capitalization %, punctuation density)
- **Model outputs** were saved as OOF predictions for meta-model training.
- The **meta-learner (LightGBM)** used early stopping and hyperparameter tuning via Optuna.

---

## 🔮 Future Improvements

- Integrate transformer-based embeddings (e.g., **DeBERTa**, **RoBERTa**)  
- Optimize meta-model via **Bayesian Optimization** or **Optuna**  
- Explore **feature selection** or **dimensionality reduction** for n-grams  
- Experiment with **calibrated probabilities** for blending stability  

---

## 🧰 Requirements

Install dependencies using:

```bash
pip install -r requirements.txt
```

### Example requirements.txt:
- numpy
- pandas
- scikit-learn
- xgboost
- lightgbm
- matplotlib
- seaborn
- optuna

---

## 👨‍💻 Authors
**Gaurav Singh** & **Ayush Gahtori**

---

## 🏆 Acknowledgements

Special thanks to:  
- The **Mercor AI** team for hosting the competition  
- The **Kaggle** community for discussions and shared insights  
- Open-source contributors of **scikit-learn**, **XGBoost**, and **LightGBM**

> *"Ensemble learning is not about a single model being perfect — it's about many models being imperfect in complementary ways."* 🌟