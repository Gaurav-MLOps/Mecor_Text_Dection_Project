# ðŸ§  Mercor AI Text Detection â€” Ensemble Learning Pipeline

This repository contains a complete *stacking and blending ensemble* solution for the [Mercor AI Text Detection](https://www.kaggle.com/competitions/mercor-ai-text-detection) Kaggle competition.  
It combines traditional machine learning models with meta-learning to achieve strong leaderboard performance.

---

## ðŸš€ Overview

The goal of the competition is to identify whether a given answer was generated by AI or written by a human.  
Our approach focuses on:
- *TF-IDF vectorization* with n-gram features  
- *Multiple base models* trained on text + engineered features  
- *Stacking and blending* to improve generalization  
- *Reproducible results* with organized notebooks and outputs

---

## ðŸ§  Ensemble Architecture

**Text Data â†’ TF-IDF Vectorization â†’ [LR | RF | XGB | LGBM] â†’ Meta-Learner (LightGBM) â†’ Weighted Blending â†’ Final Submission**

This multi-stage ensemble leverages diversity among traditional ML models and meta-learning to capture both lexical and statistical text patterns.

---

## âš™ Pipeline Highlights

| Stage | Description |
|-------|--------------|
| *1. Data Preprocessing* | Cleaning text, feature engineering (text_length, num_words, punctuation counts, etc.) |
| *2. TF-IDF Vectorization* | 1â€“2 gram TF-IDF embeddings (max 8,000 features) |
| *3. Base Models* | Logistic Regression, Random Forest, XGBoost, LightGBM |
| *4. Meta-Model (Stacking)* | LightGBM / Logistic Regression trained on base model outputs |
| *5. Blending* | Weighted average of stacked + blended outputs |

---

## ðŸ§© Results

| Metric | Score |
|---------|--------|
| *Best Model* | Stacked LightGBM Meta-Learner |
| *Public Leaderboard* | *0.95384* |
| *Validation Strategy* | 5-Fold Stratified Cross-Validation |

*Final generated submissions:*
- stacked_submission.csv
- stacked_submission_2.csv
- super_blend_submission.csv

ðŸ“… Project completed: October 2025

---

## ðŸ“‚ Project Structure
```
Mercor_AI_Text_Detection/
â”‚
â”œâ”€â”€ Mecor_Data/
â”‚ â”œâ”€â”€ train.csv
â”‚ â”œâ”€â”€ test.csv
â”‚
â”œâ”€â”€ 01_baseline_ensemble.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
---

## ðŸ“¥ Data Setup

The Kaggle dataset is *not included* due to licensing restrictions.

To reproduce results:
1. Download the dataset from the [Mercor AI Text Detection competition](https://www.kaggle.com/competitions/mercor-ai-text-detection).  
2. Create the folder structure shown above.  
3. Run the notebooks â€” everything else (preprocessing, training, and blending) will execute automatically.

---

## ðŸ Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/Mercor_AI_Text_Detection.git
```
---

# Install dependencies
pip install -r requirements.txt

---

# Run notebook
jupyter notebook 01_baseline_ensemble.ipynb

---

## âš™ï¸ Reproducibility Details

| Parameter | Value |
|------------|--------|
| **Random Seed** | 42 |
| **Cross-Validation** | StratifiedKFold (n_splits=5) |
| **TF-IDF Settings** | ngram_range=(1,2), max_features=8000 |
| **Environment** | Python 3.10, scikit-learn 1.5+, LightGBM 4.3+, XGBoost 2.1+ |

Blending weights were tuned via **grid search** on out-of-fold predictions to minimize **log loss** and maximize leaderboard stability.

---

## ðŸ”¬ Technical Notes

- **Feature engineering included:**
  - Text statistics (`text_length`, `avg_word_length`, `num_stopwords`, etc.)
  - Stylometric ratios (e.g., capitalization %, punctuation density)
- **Model outputs** were saved as OOF predictions for meta-model training.
- The **meta-learner (LightGBM)** used early stopping and hyperparameter tuning via Optuna.

---

## ðŸ”® Future Improvements

- Integrate transformer-based embeddings (e.g., **DeBERTa**, **RoBERTa**)  
- Optimize meta-model via **Bayesian Optimization** or **Optuna**  
- Explore **feature selection** or **dimensionality reduction** for n-grams  
- Experiment with **calibrated probabilities** for blending stability  

---

## ðŸ§° Requirements

Install dependencies using:

```bash
pip install -r requirements.txt
```

### Example requirements.txt:
- numpy
- pandas
- scikit-learn
- xgboost
- lightgbm
- matplotlib
- seaborn
- optuna

---

## ðŸ‘¨â€ðŸ’» Authors
**Gaurav Singh** & **Ayush Gahtori**

---

## ðŸ† Acknowledgements

Special thanks to:  
- The **Mercor AI** team for hosting the competition  
- The **Kaggle** community for discussions and shared insights  
- Open-source contributors of **scikit-learn**, **XGBoost**, and **LightGBM**

> *"Ensemble learning is not about a single model being perfect â€” it's about many models being imperfect in complementary ways."* ðŸŒŸ